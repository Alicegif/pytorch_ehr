{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* This notebook is to demonstrate how to use Bayesian Optimization package on github <https://github.com/fmfn/BayesianOptimization> locally to tune hyperparamters for our models(LR, embedding dimension=2) for predicting heart failure onset risk on cerner sample data\n",
    "* For this demonstration, the data is the original 1 hospital (h143) previously used by retain, with 42,729 patients in total\n",
    "* The logistic regression model has the architecture of an embedding layer (embedding dimension =2), a linear activation and sigmoid transformation. The hyperparameters to be tuned are: learning rate: lr, and l2 regularization \n",
    "* To implement this, first you need to install the package: however we modify the package file a bit to bypass error and keep on iterating. The modified files could be found at Experiments/modifiedBO\n",
    "* Then **important**: you need to define a function (in our case LR_tune()) which takes in the hyperparameters: l2, lr on logscale, run the model using models, Loaddata, and TrainVaTe modules and return the best validation auc\n",
    "* Be ware that this BO package will search float parameters, so if you have int or categorical parameters you want to tune, you might want to transform those values in your function before giving those to your models\n",
    "* Then **important**: call BO function and pass your LR_tune(), a search range for each parameter ((-16, 1) means -16 and 1 inclusive), and give it points to explore (points that will give you large target values) if you want to, and call maximize() and pass number of iterations you want to run BO\n",
    "* Then you will get results of your initial designated explored points(if any), 5 initializations, and plus number of BO iterations\n",
    "* For our results: it improved our best validation auc **from manually tuned 0.76980** (l2 = np.exp(-11), lr = np.exp(-9)) to **0.78329** (l2 = np.exp(-9.0790), lr = np.exp(-7.3835))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model as model \n",
    "import Loaddata as Loaddata\n",
    "import TrainVaTe as TVT\n",
    "\n",
    "# check GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preparing data...\n",
      "\n",
      "Sample data after split:\n",
      "[0, [1667, 144, 62, 85, 1667, 144, 62, 85]]\n",
      "model is LR\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "set_x = pickle.load(open('Data/h143.visits', 'rb'), encoding='bytes')\n",
    "set_y = pickle.load(open('Data/h143.labels', 'rb'),encoding='bytes')\n",
    "\n",
    "model_x = []\n",
    "for patient in set_x:\n",
    "    model_x.append([each for visit in patient for each in visit])  \n",
    "    \n",
    "merged_set= [[set_y[i],model_x[i]] for i in range(len(set_y))]\n",
    "print(\"\\nLoading and preparing data...\")    \n",
    "train1, valid1, test1 = Loaddata.load_data(merged_set)\n",
    "print(\"\\nSample data after split:\")  \n",
    "print(train1[0])\n",
    "print(\"model is\", 'LR') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#function to record comprehensive searching results \n",
    "def print2file(buf, outFile):\n",
    "    outfd = open(outFile, 'a')\n",
    "    outfd.write(buf + '\\n')\n",
    "    outfd.close()\n",
    "\n",
    "logFile='testLR_final.log'                                                                                                                                                                                            \n",
    "header = 'model|emb_dim|l2|lr|BestValidAUC|TestAUC|atEpoch'\n",
    "print2file(header, logFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamters to tune for LR: l2, lr. Define a function to return the best validation AUC of the model \n",
    "def LR_tune(l2, lr):\n",
    "    #little transformations to use the searched values                                       \n",
    "    #embed_dim = 2 ** int(embed_dim) #base 2 \n",
    "    l2 = np.exp(l2) #base e\n",
    "    lr = np.exp(lr) #base e\n",
    "    ehr_model = model.EHR_LR(embed_dim = 2)  \n",
    "    if use_cuda:\n",
    "        ehr_model = ehr_model.cuda()\n",
    "    optimizer = optim.Adam(ehr_model.parameters(), lr=lr, weight_decay=l2)\n",
    "    \n",
    "    bestValidAuc = 0.0\n",
    "    bestTestAuc = 0.0\n",
    "    bestValidEpoch = 0\n",
    "    \n",
    "    for ep in range(25): \n",
    "        current_loss, train_loss, _ = TVT.train(train1, model= ehr_model, optimizer = optimizer, batch_size = 1) \n",
    "        avg_loss = np.mean(train_loss)\n",
    "        valid_auc, y_real, y_hat, _  = TVT.calculate_auc(model = ehr_model, data = valid1, which_model = 'LR', batch_size = 1)  \n",
    "        if valid_auc > bestValidAuc: \n",
    "            bestValidAuc = valid_auc\n",
    "            bestValidEpoch = ep\n",
    "            bestTestAuc, y_real, y_hat,_ = TVT.calculate_auc(model = ehr_model, data = test1, which_model = 'LR', batch_size = 1)\n",
    "\n",
    "        if ep - bestValidEpoch >10:\n",
    "            break   \n",
    "                \n",
    "    buf = '|%f |%f |%d ' % (bestValidAuc, bestTestAuc, bestValidEpoch)\n",
    "    pFile= 'LR'+'|'+'|'+str(l2)+'|'+str(lr)+'|'+buf    \n",
    "    print2file(pFile, logFile)      \n",
    "\n",
    "    return bestValidAuc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        l2 |        lr | \n",
      "    1 | 24m54s | \u001b[35m   0.76980\u001b[0m | \u001b[32m -11.0000\u001b[0m | \u001b[32m  -9.0000\u001b[0m | \n",
      "    2 | 11m32s |    0.75786 |  -14.4753 |   -5.3769 | \n",
      "    3 | 18m42s |    0.63672 |   -1.8461 |   -7.2597 | \n",
      "    4 | 18m35s |    0.53153 |    0.4616 |   -6.5303 | \n",
      "    5 | 11m17s |    0.73457 |   -7.7876 |   -4.5391 | \n",
      "    6 | 15m49s | \u001b[35m   0.77266\u001b[0m | \u001b[32m -11.8497\u001b[0m | \u001b[32m  -5.7093\u001b[0m | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        l2 |        lr | \n",
      "    7 | 25m03s |    0.65360 |  -16.0000 |  -11.0000 | \n",
      "    8 | 23m10s |    0.63442 |  -16.0000 |   -2.0000 | \n",
      "    9 | 24m42s |    0.69488 |   -6.1462 |  -11.0000 | \n",
      "   10 | 18m08s | \u001b[35m   0.77555\u001b[0m | \u001b[32m  -9.6484\u001b[0m | \u001b[32m  -7.3119\u001b[0m | \n",
      "   11 | 24m12s |    0.77318 |  -11.5415 |   -7.0490 | \n",
      "   12 | 23m43s | \u001b[35m   0.77762\u001b[0m | \u001b[32m -10.7679\u001b[0m | \u001b[32m  -7.0225\u001b[0m | \n",
      "   13 | 23m41s |    0.76936 |  -11.7624 |   -7.0274 | \n",
      "   14 | 20m13s | \u001b[35m   0.78280\u001b[0m | \u001b[32m  -9.3082\u001b[0m | \u001b[32m  -7.3345\u001b[0m | \n",
      "   15 | 17m59s |    0.77574 |   -9.3385 |   -7.3416 | \n",
      "   16 | 13m50s |    0.77735 |  -10.8571 |   -6.7104 | \n",
      "   17 | 15m48s |    0.77728 |   -9.6047 |   -7.2997 | \n",
      "   18 | 22m00s |    0.76939 |  -11.3659 |   -6.9903 | \n",
      "   19 | 22m58s |    0.77677 |   -9.2407 |   -7.3439 | \n",
      "   20 | 21m56s |    0.77576 |  -10.7099 |   -6.5075 | \n",
      "   21 | 23m17s |    0.77602 |   -9.3530 |   -7.4147 | \n",
      "   22 | 18m34s |    0.77238 |  -11.3472 |   -7.3628 | \n",
      "   23 | 20m56s |    0.77144 |  -10.6292 |   -6.4463 | \n",
      "   24 | 24m50s |    0.77291 |   -9.0548 |   -7.4202 | \n",
      "   25 | 20m52s |    0.77202 |  -11.1634 |   -7.5069 | \n",
      "   26 | 16m12s |    0.77477 |  -10.5774 |   -6.4688 | \n",
      "   27 | 25m37s |    0.77711 |   -9.2672 |   -7.3908 | \n",
      "   28 | 26m21s |    0.77498 |  -10.5265 |   -7.6174 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu8/.local/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'nit': 7, 'funcalls': 54, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-1.57621655e-05])}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   29 | 17m51s |    0.77889 |   -9.1874 |   -7.3346 | \n",
      "   30 | 26m27s |    0.77433 |  -10.5166 |   -6.5058 | \n",
      "   31 | 27m41s |    0.77909 |   -9.0877 |   -7.4054 | \n",
      "   32 | 27m46s | \u001b[35m   0.78329\u001b[0m | \u001b[32m  -9.0790\u001b[0m | \u001b[32m  -7.3835\u001b[0m | \n",
      "   33 | 20m32s |    0.77744 |   -8.8224 |   -7.3989 | \n",
      "   34 | 27m26s |    0.77310 |  -10.0964 |   -7.7817 | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jzhu8/.local/lib/python3.5/site-packages/sklearn/gaussian_process/gpr.py:457: UserWarning: fmin_l_bfgs_b terminated abnormally with the  state: {'warnflag': 2, 'nit': 5, 'funcalls': 49, 'task': b'ABNORMAL_TERMINATION_IN_LNSRCH', 'grad': array([-1.55434358e-05])}\n",
      "  \" state: %s\" % convergence_dict)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   35 | 28m30s |    0.77898 |   -8.7901 |   -7.3250 | \n",
      "   36 | 27m45s |    0.77954 |   -8.7349 |   -7.3245 | \n",
      "-----------------------------------------------------\n",
      "Final Results\n",
      "LR: 0.783290\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gp_params = {\"alpha\": 1e-4}\n",
    "\n",
    "    LRBO = BayesianOptimization(LR_tune,\n",
    "        {'l2': (-16, 1), 'lr': (-11, -2) })\n",
    "    LRBO.explore({'l2': [-11], 'lr': [-9]})\n",
    "\n",
    "    LRBO.maximize(n_iter=30, **gp_params)\n",
    "\n",
    "    print('-' * 53)\n",
    "    print('Final Results')\n",
    "    print('LR: %f' % LRBO.res['max']['max_val'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
