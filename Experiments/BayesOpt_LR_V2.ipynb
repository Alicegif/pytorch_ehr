{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "#from sklearn.datasets import make_classification\n",
    "#from sklearn.cross_validation import cross_val_score\n",
    "#from sklearn.ensemble import RandomForestClassifier as RFC\n",
    "#from sklearn.svm import SVC\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "from bayes_opt import BayesianOptimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model_gp as model\n",
    "import Loaddata as Loaddata\n",
    "import TrainVaTe as TVT\n",
    "\n",
    "# check GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preparing data...\n",
      "\n",
      "Sample data after split:\n",
      "[0, [1667, 144, 62, 85, 1667, 144, 62, 85]]\n",
      "model is LR\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "\"\"\"\n",
    "data, target = make_classification(\n",
    "    n_samples=1000,\n",
    "    n_features=45,\n",
    "    n_informative=12,\n",
    "    n_redundant=7\n",
    ")\n",
    "\"\"\"\n",
    "set_x = pickle.load(open('Data/h143.visits', 'rb'), encoding='bytes')\n",
    "set_y = pickle.load(open('Data/h143.labels', 'rb'),encoding='bytes')\n",
    "\n",
    "model_x = []\n",
    "for patient in set_x:\n",
    "    model_x.append([each for visit in patient for each in visit])  \n",
    "    \n",
    "merged_set= [[set_y[i],model_x[i]] for i in range(len(set_y))] #list of list or list of lists of lists\n",
    "print(\"\\nLoading and preparing data...\")    \n",
    "train1, valid1, test1 = Loaddata.load_data(merged_set)\n",
    "print(\"\\nSample data after split:\")  \n",
    "print(train1[0])\n",
    "print(\"model is\", 'LR') #can change afterwards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamters to tune for LR: embed_dim, l2, lr. Define a function to return the validation AUC of the model \n",
    "def LR_tune(l2, lr):\n",
    "    #little transformations to use the searched values\n",
    "    l2 = np.exp(l2)\n",
    "    lr = np.exp(lr)\n",
    "    ehr_model = model.EHR_LR()  \n",
    "    if use_cuda:\n",
    "        ehr_model = ehr_model.cuda()\n",
    "    optimizer = optim.Adam(ehr_model.parameters(), lr=lr, weight_decay=l2)\n",
    "    current_loss_allep=[]\n",
    "    all_losses_allep=[]\n",
    "    avg_losses_allep=[]\n",
    "    #train_auc_allep =[]\n",
    "    valid_auc_allep =[]\n",
    "    test_auc_allep=[]\n",
    "    \n",
    "    for ep in range(20):\n",
    "        #start = time.time()\n",
    "        current_loss, train_loss = TVT.train(train1, model= ehr_model, optimizer = optimizer, batch_size = 1) #mb=args.mb\n",
    "        avg_loss = np.mean(train_loss)\n",
    "        #train_time = timeSince(start)\n",
    "        #print ('\\n Current running on: Epoch ', ep,'Training loss:',' Average loss', avg_loss)\n",
    "        #print(train_loss, '\\n train loss plot')\n",
    "        #TVT.showPlot(train_loss)\n",
    "        #eval_start = time.time()\n",
    "        #train_auc, y_real, y_hat = TVT.calculate_auc(model= ehr_model, data = train1, which_model ='LR', batch_size = 1)\n",
    "        #print ('\\n Current running on: Epoch ', ep,' Training auc:', train_auc)\n",
    "        #TVT.auc_plot(y_real, y_hat)\n",
    "        valid_auc, y_real, y_hat  = TVT.calculate_auc(model = ehr_model, data = valid1, which_model = 'LR', batch_size = 1)\n",
    "        #print ('\\n Current running on: Epoch ', ep,' validation auc:', valid_auc)\n",
    "        #TVT.auc_plot(y_real, y_hat)\n",
    "        test_auc, y_real, y_hat = TVT.calculate_auc(model = ehr_model, data = test1, which_model = 'LR', batch_size = 1)\n",
    "        #print ('\\n Current running on: Epoch ', ep,' test auc:', test_auc)\n",
    "        #TVT.auc_plot(y_real, y_hat)\n",
    "        #eval_time = timeSince(eval_start)\n",
    "        #print (\"Epoch \", ep, \"Summary:  Training_auc :\", train_auc, \" , Validation_auc : \", valid_auc, \" ,& Test_auc : \" , test_auc, \" Avg Loss: \", avg_loss )\n",
    "        current_loss_allep.append(current_loss)\n",
    "        all_losses_allep.append(train_loss)\n",
    "        avg_losses_allep.append(avg_loss)\n",
    "        #train_auc_allep.append(train_auc)\n",
    "        valid_auc_allep.append(valid_auc)\n",
    "        test_auc_allep.append(test_auc)\n",
    "        final_max_valid = max(valid_auc_allep)\n",
    "    return final_max_valid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        l2 |        lr | \n",
      "    1 | 21m00s | \u001b[35m   0.76808\u001b[0m | \u001b[32m -11.0000\u001b[0m | \u001b[32m  -9.0000\u001b[0m | \n",
      "    2 | 18m09s |    0.72876 |   -9.2810 |   -3.8796 | \n",
      "    3 | 16m18s |    0.73583 |   -3.2857 |   -9.4977 | \n",
      "    4 | 17m49s |    0.65022 |   -5.4683 |   -2.9056 | \n",
      "    5 | 20m02s |    0.52860 |   -0.4957 |   -8.2555 | \n",
      "    6 | 18m06s |    0.61068 |   -1.5312 |   -6.7958 | \n",
      "\u001b[31mBayesian Optimization\u001b[0m\n",
      "\u001b[94m-----------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |        l2 |        lr | \n",
      "    7 | 15m35s |    0.60945 |  -16.0000 |   -2.0000 | \n",
      "    8 | 15m37s |    0.63542 |  -16.0000 |  -11.0000 | \n",
      "    9 | 15m10s |    0.61660 |   -7.5678 |  -11.0000 | \n",
      "   10 | 15m35s |    0.75438 |  -16.0000 |   -6.5960 | \n",
      "   11 | 15m47s | \u001b[35m   0.77169\u001b[0m | \u001b[32m  -6.6973\u001b[0m | \u001b[32m  -6.9413\u001b[0m | \n",
      "   12 | 15m33s |    0.76444 |  -12.7978 |   -6.1913 | \n",
      "   13 | 15m23s | \u001b[35m   0.78146\u001b[0m | \u001b[32m  -9.8993\u001b[0m | \u001b[32m  -6.9196\u001b[0m | \n",
      "   14 | 15m53s |    0.76388 |  -13.7063 |   -8.3084 | \n",
      "   15 | 15m05s |    0.77339 |   -4.8618 |   -7.7497 | \n",
      "   16 | 15m27s |    0.77209 |  -11.3364 |   -7.3731 | \n",
      "   17 | 16m07s |    0.77844 |   -8.6632 |   -7.3758 | \n",
      "   18 | 17m38s |    0.77869 |   -7.7042 |   -5.9179 | \n",
      "   19 | 15m45s |    0.77110 |   -5.5239 |   -6.4081 | \n",
      "   20 | 15m10s |    0.78068 |   -9.6747 |   -6.5080 | \n",
      "   21 | 15m40s |    0.76941 |  -11.2614 |   -7.4315 | \n",
      "   22 | 15m37s | \u001b[35m   0.78584\u001b[0m | \u001b[32m  -5.8401\u001b[0m | \u001b[32m  -7.8810\u001b[0m | \n",
      "   23 | 15m32s |    0.78554 |   -7.7294 |   -7.1804 | \n",
      "   24 | 15m41s |    0.77872 |   -5.7776 |   -7.8462 | \n",
      "   25 | 15m06s |    0.78049 |   -8.3242 |   -6.6823 | \n",
      "   26 | 15m23s |    0.78550 |   -7.1862 |   -7.1884 | \n",
      "-----------------------------------------------------\n",
      "Final Results\n",
      "LR: 0.785845\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    gp_params = {\"alpha\": 1e-4}\n",
    "\n",
    "    LRBO = BayesianOptimization(LR_tune,\n",
    "        {'l2': (-16, 1), 'lr': (-11, -2) })\n",
    "    LRBO.explore({'l2': [-11], 'lr': [-9]})\n",
    "\n",
    "    LRBO.maximize(n_iter=20, **gp_params)\n",
    "\n",
    "    print('-' * 53)\n",
    "    print('Final Results')\n",
    "    print('LR: %f' % LRBO.res['max']['max_val'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
