{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is to demonstrate how to use SigOpt software for hyperparameter tuning Logistic Regression model:\n",
    "\n",
    "* After importing necessary modules, first create a connection to SigOpt API, you will need an account that you can sign up here: <https://sigopt.com/edu> and get the client token under the tab **API Tokens**\n",
    "* Do the normal steps of Bayesian Optimization up till the creation of the function model_tune() which takes in the hyperparameters and return the best validation AUC (sidenote: since everything will be recorded at SigOpt API and you can download it as csv file later, you can get rid of recording searched parameters on your own)\n",
    "* Then **important**: create an experiment with the connection to SigOpt API specifying the experiment name, the hypreparameters searching space, the type of hyperparameters (int, float, categorical), and finally observation budget (which is the number of iterations). I have all types in here already, so you can just copy as needed\n",
    "* You can print out the experiment id link so you can easily check out experiment results\n",
    "* Then **important**: you will need an function to pass the parameters you created in the experiments to model_tune(). Assignments are the default results from experiments, so you need to call 'assignments' dictionary for different hyperparameters\n",
    "* Then **important**: create iterations(observations), the API will automatically update suggestions based on best result\n",
    "* Then fetch the best experiment results after all iterations are done, but you can also manually retrieve the result at the experiment id link above\n",
    "* Additional infor that might be need: delete an experiment (free trial account has 10 experiments limit, so be sure to manage your unwanted ones.\n",
    "* The result: SigOpt gives a best validation auc of 0.797 so far. [see here](https://sigopt.com/guest?guest_token=BOSCVSNZUBMTYGXUPZNOQPNZQXGTVKIZVSRMTNHZBCALPQJK)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "from __future__ import division\n",
    "\n",
    "import string\n",
    "import re\n",
    "import random\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import argparse\n",
    "import time\n",
    "import math\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import torch.nn.functional as F\n",
    "from torchviz import make_dot, make_dot_from_trace\n",
    "\n",
    "from sklearn.metrics import roc_auc_score  \n",
    "from sklearn.metrics import roc_curve \n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.ticker as ticker\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    import cPickle as pickle\n",
    "except:\n",
    "    import pickle\n",
    "\n",
    "\n",
    "#from bayes_opt import BayesianOptimization\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sigopt import Connection\n",
    "conn = Connection(client_token='QULCGRMSXQAMRSTUPJPLAHDCHJMEMLXPSPZSQMWNEYKHLYJF')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.backends.cudnn.enabled=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import model_lr1 as model #this changed\n",
    "import Loaddata_final as Loaddata\n",
    "import TrainVaTe_lr1 as TVT\n",
    "\n",
    "# check GPU availability\n",
    "use_cuda = torch.cuda.is_available()\n",
    "use_cuda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading and preparing data...\n",
      "\n",
      "Sample data after split:\n",
      "[0, [491, 129, 813, 149, 84, 85, 491, 129, 813, 61, 84, 85, 358, 61, 84, 85, 491, 1250, 813, 61, 84, 85, 499, 61, 84, 85, 499, 61, 84, 85, 8004, 61, 84, 85, 2914, 61, 84, 85, 2914, 61, 84, 85, 2499, 499, 61, 84, 85, 499, 61, 84, 85, 1250, 129, 813, 61, 84, 85, 1250, 61, 84, 85]]\n",
      "model is LR1\n"
     ]
    }
   ],
   "source": [
    "# Load data set and target values\n",
    "set_x = pickle.load(open('Data/h143.visits', 'rb'), encoding='bytes')\n",
    "set_y = pickle.load(open('Data/h143.labels', 'rb'),encoding='bytes')\n",
    "\n",
    "\n",
    "#if args.which_model == 'LR':\n",
    "model_x = []\n",
    "for patient in set_x:\n",
    "    model_x.append([each for visit in patient for each in visit])  \n",
    "    \n",
    "#else: \n",
    "#    model_x = set_x  #this is for the rest of the models\n",
    "    \n",
    "merged_set= [[set_y[i],model_x[i]] for i in range(len(set_y))] #list of list or list of lists of lists\n",
    "print(\"\\nLoading and preparing data...\")    \n",
    "train1, valid1, test1 = Loaddata.load_data(merged_set)\n",
    "print(\"\\nSample data after split:\")  \n",
    "print(train1[1])\n",
    "print(\"model is\", 'LR1') #can change afterwards, currently on most basic RNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs =50 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hyperparamters to tune for LR: embed_dim, l2, lr. Define a function to return the validation AUC of the model \n",
    "def model_tune(embdim_exp, lr,l2, opt_code):\n",
    "    #little transformations to use the searched values\n",
    "    embed_dim = 2 ** int(embdim_exp) #base 2 \n",
    "    lr = np.exp(lr) \n",
    "    l2 = np.exp(l2)\n",
    "    ehr_model = model.EHR_LR(input_size =20000, embed_dim = embed_dim)  \n",
    "    if use_cuda:\n",
    "        ehr_model = ehr_model.cuda()\n",
    "    \n",
    "    if opt_code == 'Adadelta':\n",
    "        opt= 'Adadelta'\n",
    "        optimizer = optim.Adadelta(ehr_model.parameters(), lr=lr, weight_decay=l2) ## rho=0.9\n",
    "    elif opt_code == 'Adagrad':\n",
    "        opt= 'Adagrad'\n",
    "        optimizer = optim.Adagrad(ehr_model.parameters(), lr=lr, weight_decay=l2) ##lr_decay no eps\n",
    "    elif opt_code =='Adam':\n",
    "        opt= 'Adam'\n",
    "        optimizer = optim.Adam(ehr_model.parameters(), lr=lr, weight_decay=l2) ## Beta defaults (0.9, 0.999), amsgrad (false)\n",
    "    elif opt_code =='Adamax':\n",
    "        opt= 'Adamax'\n",
    "        optimizer = optim.Adamax(ehr_model.parameters(), lr=lr, weight_decay=l2 ) ### Beta defaults (0.9, 0.999)\n",
    "    elif opt_code =='RMSprop':\n",
    "        opt= 'RMSprop'\n",
    "        optimizer = optim.RMSprop(ehr_model.parameters(), lr=lr, weight_decay=l2 )                \n",
    "    elif opt_code =='ASGD':\n",
    "        opt= 'ASGD'\n",
    "        optimizer = optim.ASGD(ehr_model.parameters(), lr=lr, weight_decay=l2 ) ### other parameters\n",
    "    elif opt_code =='SGD':\n",
    "        opt= 'SGD'\n",
    "        optimizer = optim.SGD(ehr_model.parameters(), lr=lr, weight_decay=l2 ) ### other parameters\n",
    "     \n",
    "    \n",
    "    #optimizer = optim.Adam(ehr_model.parameters(), lr=lr, weight_decay=l2)\n",
    "\n",
    "    \n",
    "    bestValidAuc = 0.0\n",
    "    bestTestAuc = 0.0\n",
    "    bestValidEpoch = 0\n",
    "\n",
    "    for ep in range(epochs):\n",
    "        current_loss, train_loss, _  = TVT.train(train1, model= ehr_model, optimizer = optimizer, batch_size = 1)\n",
    "        avg_loss = np.mean(train_loss)\n",
    "        valid_auc, y_real, y_hat, _  = TVT.calculate_auc(model = ehr_model, data = valid1, which_model = 'LR', batch_size = 1)\n",
    "        if valid_auc > bestValidAuc: \n",
    "            bestValidAuc = valid_auc\n",
    "            bestValidEpoch = ep\n",
    "            best_model= ehr_model\n",
    "            #bestTestAuc, y_real, y_hat,_ = TVT.calculate_auc(model = ehr_model, data = test1, which_model = 'LR', batch_size = 1)\n",
    "\n",
    "        if ep - bestValidEpoch > 12:\n",
    "            break    \n",
    "    bestTestAuc, y_real, y_hat,_ = TVT.calculate_auc(model = ehr_model, data = test1, which_model = 'LR', batch_size = 1)\n",
    "    \n",
    "    print(bestTestAuc)\n",
    "    return bestValidAuc    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 different experiments for 3 different cells, keep the optimizer categorical \n",
    "experiment = conn.experiments().create(\n",
    "  name='LR_AUC_Optimization',\n",
    "  parameters=[\n",
    "    dict(name='embdim_exp', type='int', bounds=dict(min=5, max=9)),\n",
    "    #dict(name='hid_exp', type='int', bounds=dict(min=5, max=9)),\n",
    "    #dict(name='layers_n', type='int', bounds=dict(min=1, max=3)),\n",
    "    #dict(name='dropout', type='double', bounds=dict(min=0.1000, max= 0.9000)),\n",
    "    dict(name='lr', type='double', bounds=dict(min=-11, max= -2)), \n",
    "    dict(name='l2', type='double', bounds=dict(min=-16, max= -1)),\n",
    "    #dict(name='eps_exp', type='int', bounds=dict(min=-9, max=-4)),  \n",
    "    dict(name='opt_code', type='categorical', categorical_values=[dict(name='Adadelta'), dict(name='Adagrad'),\n",
    "                                                                  dict(name='Adam'), dict(name='Adamax'),dict(name='RMSprop'),\n",
    "                                                                  dict(name='ASGD'), dict(name='SGD')]),\n",
    "  ],\n",
    "  observation_budget=100 #10~20 times number of parameters \n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created experiment: https://sigopt.com/experiment/45007\n"
     ]
    }
   ],
   "source": [
    "print(\"Created experiment: https://sigopt.com/experiment/\" + experiment.id);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate your model with the suggested parameter assignments\n",
    "def evaluate_model(assignments):\n",
    "  return model_tune(assignments['embdim_exp'],# assignments['hid_exp'],assignments['layers_n'],assignments['dropout'],\n",
    "                    assignments['lr'],assignments['l2'],assignments['opt_code'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7289024297715341\n",
      "0.5242180346499502\n",
      "0.7679322344287479\n",
      "0.5055706456338653\n",
      "0.605624096963513\n",
      "0.6009606690132131\n",
      "0.7826039513463725\n",
      "0.6389374240076016\n",
      "0.7007067514019587\n",
      "0.6064070345267578\n",
      "0.6860350344843339\n",
      "0.707919145583691\n",
      "0.756779929277269\n",
      "0.5042398013476099\n",
      "0.6447505110578032\n",
      "0.5781859970297867\n",
      "0.5689439208392743\n",
      "0.5811023646672433\n",
      "0.5243373517238903\n",
      "0.5531279157897487\n",
      "0.7414195726776275\n",
      "0.6513978658084721\n",
      "0.6652848775024971\n",
      "0.7870078731592198\n",
      "0.8023906654479781\n",
      "0.7062425197372876\n",
      "0.7894163783793927\n",
      "0.7877300983879754\n",
      "0.5503083615501784\n",
      "0.7054986066757188\n",
      "0.7786493551982948\n",
      "0.799838231882773\n",
      "0.8067295917493229\n",
      "0.7874501281825705\n",
      "0.7693214794936442\n",
      "0.716901851610624\n",
      "0.7995258998954906\n",
      "0.80113134341915\n",
      "0.7566669349884436\n",
      "0.7976162148300177\n",
      "0.7878136543332018\n",
      "0.7853957669157617\n",
      "0.798390110130679\n",
      "0.8012011660031595\n",
      "0.7819969639753539\n",
      "0.7913989454410272\n",
      "0.801101973062488\n",
      "0.8036554944186804\n",
      "0.807061368000507\n",
      "0.7928807887137334\n"
     ]
    }
   ],
   "source": [
    "#run the experiments, no need to modify\n",
    "for i in range(100):\n",
    "  suggestion = conn.experiments(experiment.id).suggestions().create()\n",
    "  value = evaluate_model(suggestion.assignments)\n",
    "  conn.experiments(experiment.id).observations().create(\n",
    "    suggestion=suggestion.id,\n",
    "    value=value,\n",
    "  )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Wrapping up the experiments and get the results\n",
    "best_assignments_list = (\n",
    "    conn.experiments(experiment.id)\n",
    "        .best_assignments()\n",
    "        .fetch()\n",
    ")\n",
    "if best_assignments_list.data:\n",
    "    best_assignments = best_assignments_list.data[0].assignments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#see final result \n",
    "best_assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#delete an expriment \n",
    "experiment = conn.experiments('45006').delete()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tuned model\n",
    "classifier = model_tune(assignments)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
